<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Resources - BAISH - Buenos Aires AI Safety Hub</title>
    <meta name="description" content="Learning resources and past newsletters from the BAISH - Buenos Aires AI Safety Hub">
    <!-- Open Graph tags for social sharing -->
    <meta property="og:title" content="Resources - BAISH - Buenos Aires AI Safety Hub">
    <meta property="og:description" content="Learning resources and past newsletters from the BAISH - Buenos Aires AI Safety Hub">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://lucadeleo.github.io/aisafetysite/resources.html">
    <!-- Favicon -->
    <link rel="icon" href="img/favicon.ico">
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Source+Serif+Pro:wght@400;600&display=swap" rel="stylesheet">
    <!-- Custom CSS -->
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <header class="site-header">
        <div class="container">
            <div class="logo">
                <a href="index.html">BAISH - Buenos Aires AI Safety Hub</a>
            </div>
            <nav class="main-nav">
                <ul>
                    <li><a href="about.html">About</a></li>
                    <li><a href="activities.html">Activities</a></li>
                    <li><a href="resources.html" class="active">Resources</a></li>
                    <li><a href="contact.html">Contact</a></li>
                </ul>
            </nav>
            <a href="contact.html" class="btn btn-primary">Join Us</a>
        </div>
    </header>

    <section class="page-header">
        <div class="container">
            <div class="breadcrumb">
                <a href="index.html">Home</a> / <span>Resources</span>
            </div>
            <h1>Learning Resources</h1>
            <p class="subtitle">Curated materials for exploring AI safety concepts and our newsletter archive</p>
        </div>
    </section>

    <section class="learning-pathways">
        <div class="container">
            <h2>Learning Pathways</h2>
            <p class="section-intro">We've organized resources into three learning paths depending on your background and experience level.</p>

            <div class="pathways-container">
                <div class="pathway">
                    <div class="pathway-header">
                        <h3>Beginners</h3>
                        <span class="difficulty beginner">Accessible to All</span>
                    </div>
                    <p class="pathway-description">For those new to AI safety concepts who want to understand the key ideas and concerns.</p>
                    <ul class="resource-list">
                        <li>
                            <a href="https://futureoflife.org/ai-risk/" target="_blank" rel="noopener noreferrer">Future of Life Institute: AI Risk Introduction</a>
                            <span class="resource-type">Overview</span>
                        </li>
                        <li>
                            <a href="https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/" target="_blank" rel="noopener noreferrer">Cold Takes: Transformative AI series</a>
                            <span class="resource-type">Blog Series</span>
                        </li>
                        <li>
                            <a href="https://www.youtube.com/watch?v=pYXy-A4siMw" target="_blank" rel="noopener noreferrer">Rob Miles: AI Safety Intro</a>
                            <span class="resource-type">Video</span>
                        </li>
                        <li>
                            <a href="https://80000hours.org/problem-profiles/artificial-intelligence/" target="_blank" rel="noopener noreferrer">80,000 Hours: AI Risk Problem Profile</a>
                            <span class="resource-type">Overview</span>
                        </li>
                        <li>
                            <a href="https://www.aisafety.info/" target="_blank" rel="noopener noreferrer">Stampy's AI Safety Wiki</a>
                            <span class="resource-type">Wiki</span>
                        </li>
                    </ul>
                    <a href="contact.html#beginner-resources" class="btn btn-secondary">Get Beginner Recommendations</a>
                </div>

                <div class="pathway">
                    <div class="pathway-header">
                        <h3>Intermediate</h3>
                        <span class="difficulty intermediate">Some Technical Background</span>
                    </div>
                    <p class="pathway-description">For those with some understanding of machine learning who want to explore core AI safety challenges.</p>
                    <ul class="resource-list">
                        <li>
                            <a href="https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ" target="_blank" rel="noopener noreferrer">AGI Safety Fundamentals Curriculum</a>
                            <span class="resource-type">Course Materials</span>
                        </li>
                        <li>
                            <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-safety-fundamentals-technical-alignment-curriculum" target="_blank" rel="noopener noreferrer">Technical Alignment Fundamentals</a>
                            <span class="resource-type">Reading List</span>
                        </li>
                        <li>
                            <a href="https://www.anthropic.com/index/core-views-on-ai-safety" target="_blank" rel="noopener noreferrer">Anthropic: Core Views on AI Safety</a>
                            <span class="resource-type">Company Perspective</span>
                        </li>
                        <li>
                            <a href="https://arxiv.org/abs/2209.00626" target="_blank" rel="noopener noreferrer">Training Language Models to Follow Instructions</a>
                            <span class="resource-type">Research Paper</span>
                        </li>
                        <li>
                            <a href="https://www.deeplearning.ai/short-courses/alignment-techniques-for-llms/" target="_blank" rel="noopener noreferrer">DeepLearning.AI: Alignment Techniques for LLMs</a>
                            <span class="resource-type">Course</span>
                        </li>
                    </ul>
                    <a href="contact.html#intermediate-resources" class="btn btn-secondary">Get Intermediate Recommendations</a>
                </div>

                <div class="pathway">
                    <div class="pathway-header">
                        <h3>Advanced</h3>
                        <span class="difficulty advanced">Strong Technical Background</span>
                    </div>
                    <p class="pathway-description">For those with strong ML/AI knowledge who want to engage with cutting-edge research.</p>
                    <ul class="resource-list">
                        <li>
                            <a href="https://distill.pub/2020/circuits/" target="_blank" rel="noopener noreferrer">Distill: Circuits Thread</a>
                            <span class="resource-type">Interpretability Research</span>
                        </li>
                        <li>
                            <a href="https://transformer-circuits.pub/" target="_blank" rel="noopener noreferrer">Anthropic: Transformer Circuits</a>
                            <span class="resource-type">Research Series</span>
                        </li>
                        <li>
                            <a href="https://arxiv.org/abs/2212.03827" target="_blank" rel="noopener noreferrer">Discovering Latent Knowledge in Language Models</a>
                            <span class="resource-type">Research Paper</span>
                        </li>
                        <li>
                            <a href="https://arxiv.org/abs/2312.11805" target="_blank" rel="noopener noreferrer">Language Agents for Alignment Research</a>
                            <span class="resource-type">Research Paper</span>
                        </li>
                        <li>
                            <a href="https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to" target="_blank" rel="noopener noreferrer">Alignment Forum: Advanced Concepts</a>
                            <span class="resource-type">Technical Discussion</span>
                        </li>
                    </ul>
                    <a href="contact.html#advanced-resources" class="btn btn-secondary">Get Advanced Recommendations</a>
                </div>
            </div>
        </div>
    </section>

    <section id="papers" class="reading-list alt-bg">
        <div class="container">
            <h2>Paper Reading Club List</h2>
            <p class="section-intro">Papers we have discussed or plan to discuss in our Paper Reading Club.</p>

            <div class="papers-container">
                <table class="papers-table">
                    <thead>
                        <tr>
                            <th>Paper Title</th>
                            <th>Authors</th>
                            <th>Year</th>
                            <th>Category</th>
                            <th>Status</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><a href="https://arxiv.org/abs/2202.05262" target="_blank" rel="noopener noreferrer">Training language models to follow instructions with human feedback</a></td>
                            <td>Ouyang et al.</td>
                            <td>2022</td>
                            <td>Alignment</td>
                            <td>Discussed</td>
                        </tr>
                        <tr>
                            <td><a href="https://arxiv.org/abs/2212.03827" target="_blank" rel="noopener noreferrer">Discovering Latent Knowledge in Language Models Without Supervision</a></td>
                            <td>Burns et al.</td>
                            <td>2022</td>
                            <td>Interpretability</td>
                            <td>Discussed</td>
                        </tr>
                        <tr>
                            <td><a href="https://arxiv.org/abs/2211.09085" target="_blank" rel="noopener noreferrer">Constitutional AI: Harmlessness from AI Feedback</a></td>
                            <td>Bai et al.</td>
                            <td>2022</td>
                            <td>Alignment</td>
                            <td>Discussed</td>
                        </tr>
                        <tr>
                            <td><a href="https://arxiv.org/abs/2303.12712" target="_blank" rel="noopener noreferrer">Evaluating and Mitigating Discrimination in Language Model Decisions</a></td>
                            <td>Sorensen et al.</td>
                            <td>2023</td>
                            <td>Ethics</td>
                            <td>Discussed</td>
                        </tr>
                        <tr>
                            <td><a href="https://arxiv.org/abs/2302.08582" target="_blank" rel="noopener noreferrer">Scaling Laws for Reward Model Overoptimization</a></td>
                            <td>Gao et al.</td>
                            <td>2023</td>
                            <td>Alignment</td>
                            <td>Discussed</td>
                        </tr>
                        <tr>
                            <td><a href="https://arxiv.org/abs/2212.09251" target="_blank" rel="noopener noreferrer">Mechanistic Interpretability for Language Models</a></td>
                            <td>Elhage et al.</td>
                            <td>2022</td>
                            <td>Interpretability</td>
                            <td>Upcoming</td>
                        </tr>
                        <tr>
                            <td><a href="https://arxiv.org/abs/2305.08298" target="_blank" rel="noopener noreferrer">Language Models can Solve Computer Tasks</a></td>
                            <td>Gur et al.</td>
                            <td>2023</td>
                            <td>Capabilities</td>
                            <td>Upcoming</td>
                        </tr>
                        <tr>
                            <td><a href="https://arxiv.org/abs/2312.11805" target="_blank" rel="noopener noreferrer">Training Language Agents with Human Feedback</a></td>
                            <td>Zhao et al.</td>
                            <td>2023</td>
                            <td>Alignment</td>
                            <td>Planned</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </section>

    <section class="newsletter-archive">
        <div class="container">
            <h2>Newsletter Archive</h2>
            <p class="section-intro">Browse our past newsletters to stay updated on our activities and AI safety developments.</p>

            <div class="newsletter-container">
                <div class="newsletter-filters">
                    <div class="search-box">
                        <input type="text" placeholder="Search newsletters..." id="newsletter-search">
                    </div>
                    <div class="filter-dropdown">
                        <select id="year-filter">
                            <option value="all">All Years</option>
                            <option value="2025">2025</option>
                            <option value="2024">2024</option>
                            <option value="2023">2023</option>
                        </select>
                    </div>
                </div>

                <div class="newsletters-list">
                    <div class="newsletter-item">
                        <div class="newsletter-date">March 15, 2025</div>
                        <h3 class="newsletter-title">March Update: New Mech Interp Course & Fellowship Applications</h3>
                        <p class="newsletter-summary">Updates on our upcoming Mechanistic Interpretability course, Research Fellowship application deadlines, and highlights from recent AI safety publications.</p>
                        <a href="newsletters/2025-03-15.html" class="newsletter-link">Read Newsletter</a>
                    </div>

                    <div class="newsletter-item">
                        <div class="newsletter-date">February 10, 2025</div>
                        <h3 class="newsletter-title">February Update: New Research Projects & Paper Club Lineup</h3>
                        <p class="newsletter-summary">Introduction of new research projects on causal intervention methods, upcoming paper reading sessions, and community highlights.</p>
                        <a href="newsletters/2025-02-10.html" class="newsletter-link">Read Newsletter</a>
                    </div>

                    <div class="newsletter-item">
                        <div class="newsletter-date">January 5, 2025</div>
                        <h3 class="newsletter-title">2025 Kickoff: Planning and Goals for the Year</h3>
                        <p class="newsletter-summary">Our vision and goals for 2025, including new activities, research directions, and opportunities for student involvement.</p>
                        <a href="newsletters/2025-01-05.html" class="newsletter-link">Read Newsletter</a>
                    </div>

                    <div class="newsletter-item">
                        <div class="newsletter-date">December 15, 2024</div>
                        <h3 class="newsletter-title">Year-End Review: 2024 Achievements and Looking Forward</h3>
                        <p class="newsletter-summary">Recap of 2024 activities, research accomplishments, and preview of what's coming in 2025.</p>
                        <a href="newsletters/2024-12-15.html" class="newsletter-link">Read Newsletter</a>
                    </div>

                    <div class="newsletter-item">
                        <div class="newsletter-date">November 8, 2024</div>
                        <h3 class="newsletter-title">November Update: New Research Publications & AGISF Cohort Wrap-up</h3>
                        <p class="newsletter-summary">Highlights from recent research publications, completion of the Fall AGISF cohort, and winter activities.</p>
                        <a href="newsletters/2024-11-08.html" class="newsletter-link">Read Newsletter</a>
                    </div>
                </div>

                <div class="pagination">
                    <button class="pagination-btn active">1</button>
                    <button class="pagination-btn">2</button>
                    <button class="pagination-btn">3</button>
                    <span class="pagination-ellipsis">...</span>
                    <button class="pagination-btn">8</button>
                </div>
            </div>
        </div>
    </section>

    <section class="recommended-reading alt-bg">
        <div class="container">
            <h2>Recommended Books</h2>
            <div class="books-grid">
                <div class="book-card">
                    <div class="book-cover"></div>
                    <h3>Human Compatible</h3>
                    <p class="book-author">Stuart Russell</p>
                    <p class="book-description">A leading AI researcher's compelling case for how to ensure that artificial intelligence remains beneficial to humanity.</p>
                </div>

                <div class="book-card">
                    <div class="book-cover"></div>
                    <h3>Superintelligence</h3>
                    <p class="book-author">Nick Bostrom</p>
                    <p class="book-description">Explores the potential risks and paths to superintelligent AI, addressing key questions about the future of intelligence.</p>
                </div>

                <div class="book-card">
                    <div class="book-cover"></div>
                    <h3>The Alignment Problem</h3>
                    <p class="book-author">Brian Christian</p>
                    <p class="book-description">An exploration of the growing field of AI alignment, explaining technical concepts in an accessible way.</p>
                </div>

                <div class="book-card">
                    <div class="book-cover"></div>
                    <h3>Life 3.0</h3>
                    <p class="book-author">Max Tegmark</p>
                    <p class="book-description">Examines how artificial intelligence might affect life in the future and what choices we face in shaping that future.</p>
                </div>
            </div>
        </div>
    </section>

    <footer class="site-footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-logo">
                    <div class="logo">BAISH - Buenos Aires AI Safety Hub</div>
                    <p class="tagline">Ensuring beneficial AI through research and education.</p>
                </div>
                <div class="footer-links">
                    <div class="link-group">
                        <h4>Pages</h4>
                        <ul>
                            <li><a href="index.html">Home</a></li>
                            <li><a href="about.html">About</a></li>
                            <li><a href="activities.html">Activities</a></li>
                            <li><a href="research.html">Research</a></li>
                            <li><a href="resources.html">Resources</a></li>
                            <li><a href="contact.html">Contact</a></li>
                        </ul>
                    </div>
                    <div class="link-group">
                        <h4>Connect</h4>
                        <ul>
                            <li><a href="mailto:aisafetyarg@gmail.com">Email</a></li>
                            <li><a href="https://t.me/+zfvMHU8TaAhjNjVh">Telegram</a></li>
                            <li><a href="https://www.instagram.com/aisafetyarg">Instagram</a></li>
                        </ul>
                    </div>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 BAISH - Buenos Aires AI Safety Hub. A student initiative at the University of Buenos Aires.</p>
            </div>
        </div>
    </footer>
</body>
</html>
