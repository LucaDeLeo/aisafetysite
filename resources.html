<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Resources - BAISH - Buenos Aires AI Safety Hub</title>
    <meta name="description" content="Learning resources and past newsletters from the BAISH - Buenos Aires AI Safety Hub">
    <!-- Open Graph tags for social sharing -->
    <meta property="og:title" content="Resources - BAISH - Buenos Aires AI Safety Hub">
    <meta property="og:description" content="Learning resources and past newsletters from the BAISH - Buenos Aires AI Safety Hub">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://lucadeleo.github.io/aisafetysite/resources.html">
    <!-- Favicon -->
    <link rel="icon" href="img/favicon.ico">
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Source+Serif+Pro:wght@400;600&display=swap" rel="stylesheet">
    <!-- Custom CSS -->
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <header class="site-header">
        <div class="container">
            <div class="logo">
                <a href="index.html">BAISH - Buenos Aires AI Safety Hub</a>
            </div>
            <nav class="main-nav">
                <ul>
                    <li><a href="about.html">About</a></li>
                    <li><a href="activities.html">Activities</a></li>
                    <li><a href="resources.html" class="active">Resources</a></li>
                    <li><a href="contact.html">Contact</a></li>
                </ul>
            </nav>
            <a href="contact.html" class="btn btn-primary">Join Us</a>
        </div>
    </header>

    <section class="page-header">
        <div class="container">
            <div class="breadcrumb">
                <a href="index.html">Home</a> / <span>Resources</span>
            </div>
            <h1>Learning Resources</h1>
            <p class="subtitle">Curated materials for exploring AI safety concepts and our newsletter archive</p>
        </div>
    </section>

    <section class="learning-pathways">
        <div class="container">
            <h2>Learning Pathways</h2>
            <p class="section-intro">We've organized resources into three learning paths depending on your background and experience level.</p>

            <div class="pathways-container">
                <div class="pathway">
                    <div class="pathway-header">
                        <h3>Beginners</h3>
                        <span class="difficulty beginner">Accessible to All</span>
                    </div>
                    <p class="pathway-description">For those new to AI safety concepts who want to understand the key ideas and concerns.</p>
                    <ul class="resource-list">
                        <li>
                            <a href="https://futureoflife.org/ai-risk/" target="_blank" rel="noopener noreferrer">Future of Life Institute: AI Risk Introduction</a>
                            <span class="resource-type">Overview</span>
                        </li>
                        <li>
                            <a href="https://www.cold-takes.com/transformative-ai-timelines-part-1-of-4-what-kind-of-ai/" target="_blank" rel="noopener noreferrer">Cold Takes: Transformative AI series</a>
                            <span class="resource-type">Blog Series</span>
                        </li>
                        <li>
                            <a href="https://www.youtube.com/watch?v=pYXy-A4siMw" target="_blank" rel="noopener noreferrer">Rob Miles: AI Safety Intro</a>
                            <span class="resource-type">Video</span>
                        </li>
                        <li>
                            <a href="https://80000hours.org/problem-profiles/artificial-intelligence/" target="_blank" rel="noopener noreferrer">80,000 Hours: AI Risk Problem Profile</a>
                            <span class="resource-type">Overview</span>
                        </li>
                        <li>
                            <a href="https://www.aisafety.info/" target="_blank" rel="noopener noreferrer">Stampy's AI Safety Wiki</a>
                            <span class="resource-type">Wiki</span>
                        </li>
                    </ul>
                    <a href="contact.html#beginner-resources" class="btn btn-secondary">Get Beginner Recommendations</a>
                </div>

                <div class="pathway">
                    <div class="pathway-header">
                        <h3>Intermediate</h3>
                        <span class="difficulty intermediate">Some Technical Background</span>
                    </div>
                    <p class="pathway-description">For those with some understanding of machine learning who want to explore core AI safety challenges.</p>
                    <ul class="resource-list">
                        <li>
                            <a href="https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ" target="_blank" rel="noopener noreferrer">AGI Safety Fundamentals Curriculum</a>
                            <span class="resource-type">Course Materials</span>
                        </li>
                        <li>
                            <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-safety-fundamentals-technical-alignment-curriculum" target="_blank" rel="noopener noreferrer">Technical Alignment Fundamentals</a>
                            <span class="resource-type">Reading List</span>
                        </li>
                        <li>
                            <a href="https://www.anthropic.com/index/core-views-on-ai-safety" target="_blank" rel="noopener noreferrer">Anthropic: Core Views on AI Safety</a>
                            <span class="resource-type">Company Perspective</span>
                        </li>
                        <li>
                            <a href="https://arxiv.org/abs/2209.00626" target="_blank" rel="noopener noreferrer">Training Language Models to Follow Instructions</a>
                            <span class="resource-type">Research Paper</span>
                        </li>
                        <li>
                            <a href="https://www.deeplearning.ai/short-courses/alignment-techniques-for-llms/" target="_blank" rel="noopener noreferrer">DeepLearning.AI: Alignment Techniques for LLMs</a>
                            <span class="resource-type">Course</span>
                        </li>
                    </ul>
                    <a href="contact.html#intermediate-resources" class="btn btn-secondary">Get Intermediate Recommendations</a>
                </div>

                <div class="pathway">
                    <div class="pathway-header">
                        <h3>Advanced</h3>
                        <span class="difficulty advanced">Strong Technical Background</span>
                    </div>
                    <p class="pathway-description">For those with strong ML/AI knowledge who want to engage with cutting-edge research.</p>
                    <ul class="resource-list">
                        <li>
                            <a href="https://distill.pub/2020/circuits/" target="_blank" rel="noopener noreferrer">Distill: Circuits Thread</a>
                            <span class="resource-type">Interpretability Research</span>
                        </li>
                        <li>
                            <a href="https://transformer-circuits.pub/" target="_blank" rel="noopener noreferrer">Anthropic: Transformer Circuits</a>
                            <span class="resource-type">Research Series</span>
                        </li>
                        <li>
                            <a href="https://arxiv.org/abs/2212.03827" target="_blank" rel="noopener noreferrer">Discovering Latent Knowledge in Language Models</a>
                            <span class="resource-type">Research Paper</span>
                        </li>
                        <li>
                            <a href="https://arxiv.org/abs/2312.11805" target="_blank" rel="noopener noreferrer">Language Agents for Alignment Research</a>
                            <span class="resource-type">Research Paper</span>
                        </li>
                        <li>
                            <a href="https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to" target="_blank" rel="noopener noreferrer">Alignment Forum: Advanced Concepts</a>
                            <span class="resource-type">Technical Discussion</span>
                        </li>
                    </ul>
                    <a href="contact.html#advanced-resources" class="btn btn-secondary">Get Advanced Recommendations</a>
                </div>
            </div>
        </div>
    </section>

    <section class="recommended-reading alt-bg">
        <div class="container">
            <h2>Recommended Books</h2>
            <div class="books-grid">
                <div class="book-card">
                    <div class="book-cover"></div>
                    <h3>Uncontrollable</h3>
                    <p class="book-author">Darren McKee (2023)</p>
                    <p class="book-description">Uncontrollable uses engaging analogies and relatable examples to summarize AI for beginners, and unpacks AI risk and safety for readers without a technical background.</p>
                </div>

                <div class="book-card">
                    <div class="book-cover"></div>
                    <h3>Human Compatible</h3>
                    <p class="book-author">Stuart Russell (2019)</p>
                    <p class="book-description">A leading AI researcher's compelling case for how to ensure that artificial intelligence remains beneficial to humanity.</p>
                </div>

                <div class="book-card">
                    <div class="book-cover"></div>
                    <h3>Superintelligence</h3>
                    <p class="book-author">Nick Bostrom (2014)</p>
                    <p class="book-description">Explores the potential risks and paths to superintelligent AI, addressing key questions about the future of intelligence.</p>
                </div>

                <div class="book-card">
                    <div class="book-cover"></div>
                    <h3>The Alignment Problem</h3>
                    <p class="book-author">Brian Christian (2020)</p>
                    <p class="book-description">An exploration of the growing field of AI alignment, explaining technical concepts in an accessible way.</p>
                </div>

                <div class="book-card">
                    <div class="book-cover"></div>
                    <h3>Life 3.0</h3>
                    <p class="book-author">Max Tegmark (2017)</p>
                    <p class="book-description">Examines how artificial intelligence might affect life in the future and what choices we face in shaping that future.</p>
                </div>
            </div>
        </div>
    </section>

    <footer class="site-footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-logo">
                    <div class="logo">BAISH - Buenos Aires AI Safety Hub</div>
                    <p class="tagline">Ensuring beneficial AI through research and education.</p>
                </div>
                <div class="footer-links">
                    <div class="link-group">
                        <h4>Pages</h4>
                        <ul>
                            <li><a href="index.html">Home</a></li>
                            <li><a href="about.html">About</a></li>
                            <li><a href="activities.html">Activities</a></li>
                            <li><a href="resources.html">Resources</a></li>
                            <li><a href="contact.html">Contact</a></li>
                        </ul>
                    </div>
                    <div class="link-group">
                        <h4>Connect</h4>
                        <ul>
                            <li><a href="mailto:aisafetyarg@gmail.com">Email</a></li>
                            <li><a href="https://t.me/+zfvMHU8TaAhjNjVh">Telegram</a></li>
                            <li><a href="https://www.instagram.com/aisafetyarg">Instagram</a></li>
                        </ul>
                    </div>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 BAISH - Buenos Aires AI Safety Hub. A student initiative at the University of Buenos Aires.</p>
            </div>
        </div>
    </footer>
</body>
</html>
