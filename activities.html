<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Activities & Events - BAISH - Buenos Aires AI Safety Hub</title>
    <meta name="description" content="Explore our activities and events at the BAISH - Buenos Aires AI Safety Hub">
    <!-- Open Graph tags for social sharing -->
    <meta property="og:title" content="Activities & Events - BAISH - Buenos Aires AI Safety Hub">
    <meta property="og:description" content="Explore our activities and events at the BAISH - Buenos Aires AI Safety Hub">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://lucadeleo.github.io/aisafetysite/activities.html">
    <!-- Favicon -->
    <link rel="icon" href="img/favicon.ico">
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Source+Serif+Pro:wght@400;600&display=swap" rel="stylesheet">
    <!-- Custom CSS -->
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <header class="site-header">
        <div class="container">
            <div class="logo">
                <a href="index.html">BAISH - Buenos Aires AI Safety Hub</a>
            </div>
            <nav class="main-nav">
                <ul>
                    <li><a href="about.html">About</a></li>
                    <li><a href="activities.html" class="active">Activities</a></li>
                    <li><a href="resources.html">Resources</a></li>
                    <li><a href="contact.html">Contact</a></li>
                </ul>
            </nav>
            <a href="contact.html" class="btn btn-primary">Join Us</a>
        </div>
    </header>

    <section class="page-header">
        <div class="container">
            <div class="breadcrumb">
                <a href="index.html">Home</a> / <span>Activities</span>
            </div>
            <h1>Our Activities</h1>
            <p class="subtitle">Join our community and participate in AI safety research and learning</p>
        </div>
    </section>

    <section id="agi-fundamentals" class="activity-detail">
        <div class="container">
            <div class="activity-header">
                <div class="activity-icon">üìö</div>
                <h2>AGI Safety Fundamentals Cohort</h2>
                <span class="status active">Currently Active</span>
            </div>
            <div class="activity-content">
                <div class="activity-description">
                    <p>The AGI Safety Fundamentals cohort is an 8-week guided course covering the essential concepts in AI alignment and safety. Participants read selected materials and meet weekly to discuss the readings with a facilitator.</p>
                    <p>This program is based on the AGI Safety Fundamentals curriculum and provides a structured introduction to the field of AI safety for students with technical backgrounds.</p>

                    <h3>What to Expect</h3>
                    <ul>
                        <li>Weekly 2-hour discussion sessions</li>
                        <li>5-8 hours of reading per week</li>
                        <li>Small groups of 4-8 participants</li>
                        <li>Experienced facilitators to guide discussions</li>
                        <li>Certificate of completion</li>
                    </ul>
                </div>
                <div class="activity-sidebar">
                    <div class="activity-details-card">
                        <h3>Fellowship Details</h3>
                        <ul class="details-list">
                            <li><strong>Duration:</strong> 10-12 weeks</li>
                            <li><strong>Applications Open:</strong> April 1, 2025</li>
                            <li><strong>Application Deadline:</strong> April 30, 2025</li>
                            <li><strong>Fellowship Period:</strong> June - August 2025</li>
                            <li><strong>Stipend:</strong> Available for qualified applicants</li>
                        </ul>
                        <a href="contact.html#fellowship" class="btn btn-primary">Learn About Applying</a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="weekly-discussion" class="activity-detail">
        <div class="container">
            <div class="activity-header">
                <div class="activity-icon">üí¨</div>
                <h2>Weekly Discussion Group</h2>
                <span class="status active">Every Tuesday @ 18:00</span>
            </div>
            <div class="activity-content">
                <div class="activity-description">
                    <p>Our Weekly Discussion Group provides a casual forum for discussing recent papers, concepts, and developments in AI safety. These sessions are open to anyone interested in the field, regardless of prior knowledge.</p>
                    <p>Each week features a different topic, announced in advance through our mailing list and Telegram group.</p>

                    <h3>Format</h3>
                    <ul>
                        <li>90-minute discussions led by a rotating facilitator</li>
                        <li>Short presentation of the week's topic (15-20 minutes)</li>
                        <li>Open discussion and Q&A</li>
                        <li>Optional pre-reading materials shared in advance</li>
                    </ul>

                    <h3>Recent Topics</h3>
                    <ul>
                        <li>Interpretability Methods for Transformer Models</li>
                        <li>Constitutional AI and Safety Techniques</li>
                        <li>The Scaling Hypothesis and Its Implications</li>
                        <li>AI Governance Frameworks</li>
                        <li>Value Learning and Human Feedback</li>
                    </ul>

                    <h3>Participation</h3>
                    <p>No registration is required. Simply show up! If you're attending for the first time, we recommend arriving 10 minutes early to meet the organizers.</p>
                </div>
                <div class="activity-sidebar">
                    <div class="activity-details-card">
                        <h3>Next Discussion</h3>
                        <ul class="details-list">
                            <li><strong>Date:</strong> March 25, 2025</li>
                            <li><strong>Time:</strong> 18:00 - 19:30</li>
                            <li><strong>Location:</strong> Pavilion I, Room 114</li>
                            <li><strong>Topic:</strong> Interpretability Methods</li>
                            <li><strong>Facilitator:</strong> Carlos Mendez</li>
                        </ul>
                        <a href="https://t.me/+zfvMHU8TaAhjNjVh" class="btn btn-primary">Join Telegram for Updates</a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="paper-reading" class="activity-detail alt-bg">
        <div class="container">
            <div class="activity-header">
                <div class="activity-icon">üìù</div>
                <h2>Paper Reading Club</h2>
                <span class="status active">Every Friday @ 17:00</span>
            </div>
            <div class="activity-content">
                <div class="activity-description">
                    <p>The Paper Reading Club conducts deep dives into foundational and recent papers in AI safety research. Unlike the more casual discussion group, this activity involves a thorough examination of specific research papers.</p>
                    <p>Participants are expected to read the selected paper in advance and come prepared to discuss its methods, results, and implications.</p>

                    <h3>Paper Selection Criteria</h3>
                    <ul>
                        <li>Importance to the field of AI safety</li>
                        <li>Technical relevance to current research directions</li>
                        <li>Mix of foundational papers and recent publications</li>
                        <li>Accessibility to graduate and advanced undergraduate students</li>
                    </ul>

                    <h3>Session Format</h3>
                    <ul>
                        <li>Brief overview of the paper (5-10 minutes)</li>
                        <li>Section-by-section discussion</li>
                        <li>Examination of methods and results</li>
                        <li>Critical evaluation of claims and limitations</li>
                        <li>Discussion of potential follow-up research</li>
                    </ul>

                    <h3>Recent Papers</h3>
                    <ul>
                        <li>"Discovering Latent Knowledge in Language Models Without Supervision"</li>
                        <li>"In-Context Reinforcement Learning with Algorithm Distillation"</li>
                        <li>"A General Language Assistant as a Laboratory for Alignment"</li>
                        <li>"Training Language Models to Follow Instructions with Human Feedback"</li>
                    </ul>
                </div>
                <div class="activity-sidebar">
                    <div class="activity-details-card">
                        <h3>Next Paper Session</h3>
                        <ul class="details-list">
                            <li><strong>Date:</strong> March 21, 2025</li>
                            <li><strong>Time:</strong> 17:00 - 18:30</li>
                            <li><strong>Location:</strong> Pavilion I, Room 214</li>
                            <li><strong>Paper:</strong> "Mechanistic Interpretability for Language Models"</li>
                            <li><strong>Discussion Lead:</strong> Sofia Rodriguez</li>
                        </ul>
                        <a href="resources.html#papers" class="btn btn-primary">Access Reading List</a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="mech-interp" class="activity-detail">
        <div class="container">
            <div class="activity-header">
                <div class="activity-icon">üß†</div>
                <h2>Mech Interp Course</h2>
                <span class="status upcoming">Starts June 2025</span>
            </div>
            <div class="activity-content">
                <div class="activity-description">
                    <p>The Mechanistic Interpretability Course is an intensive 1-month program focused on techniques for understanding the internal mechanisms of neural networks. This course combines theoretical learning with hands-on projects.</p>
                    <p>Mechanistic interpretability is a key area of AI safety research, aiming to make AI systems more transparent and understandable.</p>

                    <h3>Curriculum Overview</h3>
                    <ul>
                        <li>Foundations of neural network architectures</li>
                        <li>Feature visualization techniques</li>
                        <li>Circuit analysis in transformer models</li>
                        <li>Techniques for analyzing attention mechanisms</li>
                        <li>Gradient-based attribution methods</li>
                        <li>Final project: Interpreting a specific model component</li>
                    </ul>

                    <h3>Time Commitment</h3>
                    <ul>
                        <li>2 lectures per week (2 hours each)</li>
                        <li>1 practical session per week (3 hours)</li>
                        <li>Individual project work (5-10 hours per week)</li>
                        <li>Final project presentation</li>
                    </ul>

                    <h3>Prerequisites</h3>
                    <ul>
                        <li>Strong programming skills (Python)</li>
                        <li>Experience with deep learning frameworks (PyTorch preferred)</li>
                        <li>Familiarity with basic neural network architectures</li>
                        <li>Linear algebra and calculus</li>
                    </ul>
                </div>
                <div class="activity-sidebar">
                    <div class="activity-details-card">
                        <h3>Course Details</h3>
                        <ul class="details-list">
                            <li><strong>Duration:</strong> 4 weeks</li>
                            <li><strong>Start Date:</strong> June 2, 2025</li>
                            <li><strong>End Date:</strong> June 27, 2025</li>
                            <li><strong>Application Deadline:</strong> May 15, 2025</li>
                            <li><strong>Location:</strong> Hybrid (In-person & Zoom)</li>
                            <li><strong>Instructors:</strong> Dr. Laura Fernandez, Carlos Mendez</li>
                        </ul>
                        <a href="contact.html#mech-interp" class="btn btn-primary">Express Interest</a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="calendar">
        <div class="container">
            <h2>Upcoming Events Calendar</h2>
            <div class="calendar-embed">
                <!-- Placeholder for calendar embed -->
                <div class="calendar-placeholder">
                    <p>March 2025</p>
                    <table class="mini-calendar">
                        <thead>
                            <tr>
                                <th>Mon</th>
                                <th>Tue</th>
                                <th>Wed</th>
                                <th>Thu</th>
                                <th>Fri</th>
                                <th>Sat</th>
                                <th>Sun</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td></td>
                                <td></td>
                                <td></td>
                                <td></td>
                                <td>1</td>
                                <td>2</td>
                                <td>3</td>
                            </tr>
                            <tr>
                                <td>4</td>
                                <td class="event-day">5</td>
                                <td>6</td>
                                <td>7</td>
                                <td class="event-day">8</td>
                                <td>9</td>
                                <td>10</td>
                            </tr>
                            <tr>
                                <td>11</td>
                                <td class="event-day">12</td>
                                <td>13</td>
                                <td>14</td>
                                <td class="event-day">15</td>
                                <td>16</td>
                                <td>17</td>
                            </tr>
                            <tr>
                                <td>18</td>
                                <td class="event-day">19</td>
                                <td>20</td>
                                <td>21</td>
                                <td class="event-day">22</td>
                                <td>23</td>
                                <td>24</td>
                            </tr>
                            <tr>
                                <td>25</td>
                                <td class="event-day current-day">26</td>
                                <td>27</td>
                                <td>28</td>
                                <td class="event-day">29</td>
                                <td>30</td>
                                <td>31</td>
                            </tr>
                        </tbody>
                    </table>
                    <p class="calendar-note">Full interactive calendar available via Google Calendar</p>
                </div>
            </div>
            <div class="calendar-actions">
                <a href="https://calendar.google.com/calendar/u/0?cid=Y19iNzVkM2U5ZGVlMWNiN2Y4N2U1MGQzYTk4N2FkYjc4ZGE1YmIxYzg0MTRiZDU1NDcyOTg1NTM2OWM3ZjFkMWNkQGdyb3VwLmNhbGVuZGFyLmdvb2dsZS5jb20" class="btn btn-secondary" target="_blank" rel="noopener noreferrer">Subscribe to Calendar</a>
            </div>
        </div>
    </section>

    <section id="research-fellowship" class="activity-detail alt-bg">
        <div class="container">
            <div class="activity-header">
                <div class="activity-icon">üî¨</div>
                <h2>AI Safety Research Fellowship</h2>
                <span class="status upcoming">Applications Open April 2025</span>
            </div>
            <div class="activity-content">
                <div class="activity-description">
                    <p>The AI Safety Research Fellowship supports students conducting part-time research on AI safety-related problems. Fellows work on research projects under the guidance of experienced mentors and receive a stipend to support their work.</p>
                    <p>This program aims to provide hands-on research experience and contribute to the development of novel AI safety techniques and insights.</p>

                    <h3>Fellowship Structure</h3>
                    <ul>
                        <li>10-12 week commitment (flexible hours)</li>
                        <li>Weekly 1-on-1 mentorship</li>
                        <li>Bi-weekly group check-ins with other fellows</li>
                        <li>Final presentation of research findings</li>
                        <li>Opportunity to publish or present at student conferences</li>
                    </ul>

                    <h3>Past Research Topics</h3>
                    <ul>
                        <li>Detecting deceptive behavior in language models</li>
                        <li>Interpretability techniques for transformer attention mechanisms</li>
                        <li>Evaluating robustness of alignment techniques</li>
                        <li>Value learning from human feedback</li>
                    </ul>

                    <h3>Eligibility</h3>
                    <ul>
                        <li>Current BAISH students (undergraduate or graduate)</li>
                        <li>Background in computer science, mathematics, physics, or related fields</li>
                        <li>Familiarity with machine learning concepts</li>
                        <li>Previous participation in AGI Safety Fundamentals or equivalent knowledge</li>
                    </ul>
                </div>
                <div class="activity-sidebar">
                    <div class="activity-details-card">
                        <h3>Next Cohort</h3>
                        <ul class="details-list">
                            <li><strong>Start Date:</strong> September 5, 2025</li>
                            <li><strong>Application Deadline:</strong> August 15, 2025</li>
                            <li><strong>Location:</strong> Hybrid (In-person & Zoom)</li>
                            <li><strong>Frequency:</strong> Weekly (Tuesdays, 17:00-19:00)</li>
                        </ul>
                        <a href="contact.html#apply" class="btn btn-primary">Apply Now</a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer class="site-footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-logo">
                    <div class="logo">BAISH - Buenos Aires AI Safety Hub</div>
                    <p class="tagline">Ensuring beneficial AI through research and education.</p>
                </div>
                <div class="footer-links">
                    <div class="link-group">
                        <h4>Pages</h4>
                        <ul>
                            <li><a href="index.html">Home</a></li>
                            <li><a href="about.html">About</a></li>
                            <li><a href="activities.html">Activities</a></li>
                            <li><a href="resources.html">Resources</a></li>
                            <li><a href="contact.html">Contact</a></li>
                        </ul>
                    </div>
                    <div class="link-group">
                        <h4>Connect</h4>
                        <ul>
                            <li><a href="mailto:aisafetyarg@gmail.com">Email</a></li>
                            <li><a href="https://t.me/+zfvMHU8TaAhjNjVh">Telegram</a></li>
                            <li><a href="https://www.instagram.com/aisafetyarg">Instagram</a></li>
                        </ul>
                    </div>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 BAISH - Buenos Aires AI Safety Hub. A student initiative at the University of Buenos Aires.</p>
            </div>
        </div>
    </footer>
</body>
</html>
